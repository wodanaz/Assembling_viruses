configfile: "config/config.yaml"

SAMPLES, = glob_wildcards(config["inputdir"] + "{sample}_R1_001.fastq.gz")
GENOME_INDEX_FILES = multiext(config["genome"], ".amb", ".ann", ".bwt", ".pac", ".sa", ".fai")
GENOME_INDEX_FILES.append(config["genome"].replace(".fasta", ".dict"))

# Defines the files to be created by the workflow
def get_final_output():
    final_output = []
    final_output += expand("{sample}.gatk.tab", sample=SAMPLES)
    final_output += expand("{sample}.gatk.filt.vcf", sample=SAMPLES)
    final_output.append("coverage.gatk.tab")
    final_output.append("coverage.raw.tab")
    final_output += expand("{sample}.cleaned.fasta", sample=SAMPLES)

    if "datetab" in config:
        final_output.append(config["project"] + ".csv")
        final_output.append(config["project"] + "_lineages_of_concern.csv")
        final_output.append("supermetadata.tab")
        final_output.append(config["project"] + ".fasta")
        final_output.append("results.xlsx")

    if config["mode"] != 'experimental':
        final_output += expand("{sample}.spike.tab", sample=SAMPLES)
        final_output.append("spike_depths.final.tab")
        final_output += expand("{sample}.depth.tab", sample=SAMPLES)
        final_output.append("spike_genotypes.final.tab")

    return final_output

rule all:
    input:
        get_final_output(),

rule index_reference_genome:
    message: "Index Reference Genome {input}."
    input: "{genome}"
    output: multiext("{genome}", ".amb", ".ann", ".bwt", ".pac", ".sa", ".fai"),
    conda: "envs/environment.yml"
    log: "logs/index_reference_genome/{genome}.log"
    params:
        script=srcdir("scripts/index-reference-genome.sh")
    shell: "{params.script} {input} &>{log}"

rule create_picard_dictionary:
    message: "Create Picard Dictionary {input}."
    input: "{genomename}.fasta"
    output: "{genomename}.dict"
    conda: "envs/environment.yml"
    log: "logs/create_picard_dictionary/{genomename}.log"
    params:
        script=srcdir("scripts/create-picard-dictionary.sh")
    shell: "{params.script} {input} {output} &>{log}"

rule remove_adapters:
    message: "Remove Nextera Adapters {input}."
    input: "{prefix}.fastq.gz"
    output: "{prefix}_trimmed.fq.gz"
    conda: "envs/environment.yml"
    log: "logs/remove_adapters/{prefix}.log"
    params:
         script=srcdir("scripts/remove-nextera-adapters.sh")
    shell: "{params.script} {input} &>{log}"

rule map_bwa_cleaned_libs:
    message: "Map using BWA with the cleaned libraries {input.readfile}."
    input:
        readfile="{sample}_R1_001_trimmed.fq.gz",
        genome=config["genome"],
        genome_indexes=GENOME_INDEX_FILES, # ensure genome is indexed created before running this step
    output: "{sample}.sam"
    conda: "envs/environment.yml"
    log: "logs/map_bwa_cleaned_libs/{sample}.log"
    params:
        script=srcdir("scripts/map-bwa-cleaned-libs.sh")
    shell: "{params.script} {input.readfile} {input.genome} &>{log}"

rule create_bam_from_sam:
    message: "Create BAM from SAM and make an index {input}."
    input: "{sample}.sam"
    output: "{sample}.bam"
    conda: "envs/environment.yml"
    log: "logs/create_bam_from_sam/{sample}.log"
    resources:
        mem_mb=1000
    params:
        script=srcdir("scripts/create-bam-from-sam.sh")
    shell: "{params.script} {input} &>{log}"

rule mark_duplicates:
    message: "'Deduplicate' or mark PCR duplicates {input}."
    input: "{sample}.bam"
    output:
        bam="{sample}.dedup.bam",
        metrics="{sample}.metric.txt",
    conda: "envs/environment.yml"
    log: "logs/mark_duplicates/{sample}.log"
    resources:
        mem_mb=15360 # 15G
    params:
        script=srcdir("scripts/mark-duplicates.sh")
    shell: "{params.script} {input} &>{log}"

rule convert_bam_to_vcf:
    message: "BAM TO VCF USING BCFTOOLS TO CREATE A PRELIMINAR VCF FILE {input.bamfile}."
    input:
        bamfile="{sample}.dedup.bam",
        genome=config["genome"],
        genome_indexes=GENOME_INDEX_FILES, # ensure genome is indexed created before running this step
    output: "{sample}.raw.vcf"
    conda: "envs/environment.yml"
    log: "logs/convert_bam_to_vcf/{sample}.log"
    resources:
        mem_mb=10240 # 10G
    params:
        script=srcdir("scripts/convert-bam-to-vcf.sh")
    shell: "{params.script} {input.bamfile} {input.genome} &>{log}"

rule filter_raw_snps:
    message: "Filter raw SNPs using bcftools as a template of known variable sites {input}."
    input: "{sample}.raw.vcf"
    output: "{sample}.filt.vcf.gz"
    conda: "envs/environment.yml"
    log: "logs/filter_raw_snps/{sample}.log"
    resources:
        mem_mb=10
    params:
        script=srcdir("scripts/filter-raw-snps.sh")
    shell: "{params.script} {input} &>{log}"

rule update_read_groups:
    message: "Add sme info for the read groups {input}."
    input: "{sample}.dedup.bam"
    output: "{sample}.bam2"
    conda: "envs/environment.yml"
    log: "logs/update_read_groups/{sample}.log"
    params:
        script=srcdir("scripts/update-read-groups.sh")
    shell: "{params.script} {input} &>{log}"

rule run_base_recalibration:
    message: "Base recalibration. First pass of the Base Quality Score Recalibration (BQSR) {input.bamfile}."
    input:
       bamfile="{sample}.bam2",
       genome=config["genome"],
       genome_indexes=GENOME_INDEX_FILES, # ensure genome is indexed created before running this step
       filtered_snps="{sample}.filt.vcf.gz"
    output: "{sample}.table"
    conda: "envs/environment.yml"
    log: "logs/run_base_recalibration/{sample}.log"
    resources:
        mem_mb=16384 # 16G
    params:
        script=srcdir("scripts/run-base-recalibration.sh")
    shell: "{params.script} {input.bamfile} {input.genome} &>{log}"

rule apply_bqsr:
    message: "APPLY BQSR (Apply a linear base quality recalibration model) {input.bamfile}."
    input:
       bamfile="{sample}.bam2",
       recal="{sample}.table",
       genome=config["genome"],
       genome_indexes=GENOME_INDEX_FILES, # ensure genome is indexed created before running this step
    output: "{sample}.bqsr.bam"
    conda: "envs/environment.yml"
    log: "logs/apply_bqsr/{sample}.log"
    resources:
        mem_mb=10240 # 10G
    params:
        script=srcdir("scripts/apply-bqsr.sh")
    shell: "{params.script} {input.bamfile} {input.genome} &>{log}"

rule collect_statistics:
    message: "Collect statistics: Produces a summary of alignment metrics from a BAM file {input.bamfile}."
    input:
       bamfile="{sample}.bqsr.bam",
       genome=config["genome"],
       genome_indexes=GENOME_INDEX_FILES, # ensure genome is indexed created before running this step
    output: "{sample}.stat.txt"
    conda: "envs/environment.yml"
    log: "logs/collect_statistics/{sample}.log"
    resources:
        mem_mb=4000
    params:
        script=srcdir("scripts/collect-statistics.sh")
    shell: "{params.script} {input.bamfile} {input.genome} &>{log}"

rule run_depth_coverage:
    message: "How much of the reference genome is covered by more than 1 read? {input}."
    input: "{sample}.bqsr.bam"
    output: "{sample}.depth.bed"
    conda: "envs/environment.yml"
    log: "logs/run_depth_coverage/{sample}.log"
    resources:
        mem_mb=10
    params:
        script=srcdir("scripts/run-depth-coverage.sh")
    shell: "{params.script} {input} &>{log}"

rule create_coverage_table:
    message: "Make a table with coverage information {input}."
    input: expand("{sample}.bam", sample=SAMPLES)
    output: "coverage.raw.tab"
    conda: "envs/environment.yml"
    log: "logs/create_coverage_table.log"
    params:
        script=srcdir("scripts/create-coverage-table.sh")
    shell: "{params.script} {input} &>{log}"

rule create_coverage_bqsr_table:
    message: "Make a table with BQSR coverage information {input}."
    input: expand("{sample}.bqsr.bam", sample=SAMPLES)
    output: "coverage.gatk.tab"
    conda: "envs/environment.yml"
    log: "logs/create_coverage_bqsr_table.log"
    params:
        script=srcdir("scripts/create-coverage-bqsr-table.sh")
    shell: "{params.script} {input} &>{log}"

rule run_haplotype_caller:
    message: "HAPLOTYPE CALLER: Call germline SNPs and indels via local re-assembly of haplotypes {input.bamfile}."
    input:
       bamfile="{sample}.bqsr.bam",
       genome=config["genome"],
       genome_indexes=GENOME_INDEX_FILES, # ensure genome is indexed created before running this step
    output: "{sample}.gatk.vcf"
    conda: "envs/environment.yml"
    log: "logs/run_haplotype_caller/{sample}.log"
    resources:
        mem_mb=2000
    params:
        script=srcdir("scripts/run-haplotype-caller.sh")
    shell: "{params.script} {input.bamfile} {input.genome} &>{log}"

rule filter_vcfs:
    message: "FILTER VCFs (Filter variant calls based on INFO and/or FORMAT annotations) {input.vcf}."
    input:
       vcf="{sample}.gatk.vcf",
       genome=config["genome"],
       genome_indexes=GENOME_INDEX_FILES, # ensure genome is indexed created before running this step
    output: "{sample}.gatk.filt.vcf"
    conda: "envs/environment.yml"
    log: "logs/filter_vcfs/{sample}.log"
    params:
        script=srcdir("scripts/filter-vcfs.sh")
    shell: "{params.script} {input.vcf} {input.genome} &>{log}"

rule create_genotype_table:
    message: "GENOTYPE TABLE (Extract specified fields for each variant in a VCF file to a tab-delimited table) {input.vcf}."
    input:
       vcf="{sample}.gatk.filt.vcf",
    output: "{sample}.gatk.tab"
    conda: "envs/environment.yml"
    log: "logs/create_genotype_table/{sample}.log"
    params:
        script=srcdir("scripts/create-genotype-table.sh")
    shell: "{params.script} {input.vcf} &>{log}"

rule run_bedtools_merge:
    message: "Make final consensus fasta sequence using the SNPs in the vcf file {input.bed}."
    input:
       bed="{sample}.depth.bed",
    output: "{sample}.merged.bed"
    conda: "envs/environment.yml"
    log: "logs/run_bedtools_merge/{sample}.log"
    params:
        script=srcdir("scripts/run-bedtools-merge.sh")
    shell: "{params.script} {input.bed} &>{log}"

rule run_bcftools_consensus:
    message: "Run bcftools consensus {input.vcf}."
    input:
       vcf="{sample}.gatk.filt.vcf",
       mergedbed="{sample}.merged.bed",
       genome=config["genome"],
       genome_index=GENOME_INDEX_FILES, # ensure genome is indexed created before running this step
    output: "{sample}.cleaned.fasta"
    conda: "envs/environment.yml"
    log: "logs/run_bcftools_consensus/{sample}.log"
    params:
        script=srcdir("scripts/run-bcftools-consensus.sh")
    shell: "{params.script} {input.vcf} {input.genome} &>{log}"

rule intersect_spike:
    message: "Intersect vcf files with spike.bed {input.vcf}."
    input:
       vcf="{sample}.gatk.filt.vcf",
       spike=config["spike"],
    output:
       "{sample}.depth.tab",
       "{sample}.spike.tab",
    conda: "envs/environment.yml"
    log: "logs/intersect_spike/{sample}.log"
    params:
        script=srcdir("scripts/intersect-spike.sh")
    shell: "{params.script} {input.vcf} {input.spike} &>{log}"

rule run_spike_genotype_compiler:
    message: "Run genotype compiler on spike filtered data {input}."
    input: expand("{sample}.spike.tab", sample=SAMPLES)
    output: "spike_genotypes.final.tab",
    conda: "envs/environment.yml"
    log: "logs/run_spike_genotype_compiler.log"
    params:
        script=srcdir("scripts/run-spike-genotype-compiler.sh"),
        perlscript=srcdir("scripts/genotype_compiler.pl"),
    shell: "PERLSCRIPT={params.perlscript} {params.script} {input} &>{log}"

rule run_spike_depth_compiler:
    message: "Run depth compiler on spike filtered data {input}."
    input: expand("{sample}.depth.tab", sample=SAMPLES)
    output: "spike_depths.final.tab",
    conda: "envs/environment.yml"
    log: "logs/run_spike_depth_compiler.log"
    params:
        script=srcdir("scripts/run-spike-depth-compiler.sh"),
        perlscript=srcdir("scripts/depth_compiler.pl"),
    shell: "PERLSCRIPT={params.perlscript} {params.script} {input} &>{log}"

rule run_pangolin:
    message: "Run pangolin {input}."
    input: expand("{sample}.cleaned.fasta", sample=SAMPLES)
    output:
        "consensus_sequences.fasta",
        config["project"] + ".csv",
        config["project"] + "_lineages_of_concern.csv",
    conda: "envs/environment.yml"
    log: "logs/run_pangolin.log"
    params:
        script=srcdir("scripts/run-pangolin.sh"),
        project=config['project'],
    shell: "PROJECTNAME={params.project} {params.script} {input} &>{log}"

if "datetab" in config:
    rule supermetadata_modify_titles:
        message: "Supermetadata Modify Titles."
        input:
           project=config["project"] + ".csv",
           project_lineages=config["project"] + "_lineages_of_concern.csv",
           coverage_tab="coverage.gatk.tab",
           datetab=config["datetab"],
        output:
            "supermetadata.tab",
            config["project"] + ".fasta",
        conda: "envs/environment.yml"
        log: "logs/supermetadata_modify_titles.log"
        params:
            script=srcdir("scripts/supermetadata-modify-titles.sh"),
            project=config['project'],
            mode=config['mode'],
        shell: "PROJECTNAME={params.project} EVMODE={params.mode} DATETAB={input.datetab} {params.script} &>{log}"

    rule create_spreadsheet:
        message: "Supermetadata Create Spreadsheet."
        input:
           supermeatadata="supermetadata.tab",
           spike_genotypes="spike_genotypes.final.tab",
        output:
            "results.xlsx",
        conda: "envs/environment.yml"
        log: "logs/create_spreadsheet.log"
        params:
            script=srcdir("scripts/create-spreadsheet.py"),
        shell: "{params.script} &>{log}"
